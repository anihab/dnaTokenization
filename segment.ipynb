{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw98Es3X3bdwMLSJ88eDtg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/dnaTokenization/blob/main/segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes as input a list or directory of genomic sequences in fasta format and cuts it into input segments of length *n* with length *l*.\n",
        "\n",
        "Output should be a CSV file with the following fields: sample_ID, start_position, end_position, length, sequence, label (The model will produce the label so the label should be blank)."
      ],
      "metadata": {
        "id": "rlelHQEHArTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qsZDW14AJOe",
        "outputId": "020ae861-d415-41ea-a823-a99c6d0f5b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.6.2-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.82-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.8.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.6.2 biopython-1.82 biothings-client-0.3.1 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "processes=multiprocessing.cpu_count()\n",
        "print(\"The number of processes:\")\n",
        "print(processes)\n",
        "\n",
        "\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install Bio\n",
        "from Bio import SeqIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(args):\n",
        "    f, max_length, shift_amount, output_path = args\n",
        "    filename = os.path.basename(f)\n",
        "    if not is_file_read(output_path, filename):\n",
        "        segment(f, max_length, shift_amount, output_path)\n",
        "\n",
        "\n",
        "def read_input(input_path, max_length, output_path, **kwargs):\n",
        "    shift_amount = kwargs.get('shift_amount', None)\n",
        "\n",
        "\n",
        "    files_to_process = []\n",
        "\n",
        "\n",
        "    if input_path.endswith('.txt'):  # if the input path is a list\n",
        "        if os.path.isfile(input_path):\n",
        "            with open(input_path, 'r') as list_file:\n",
        "                for f in list_file:\n",
        "                    f = f.strip()\n",
        "                    if os.path.isfile(f):\n",
        "                        files_to_process.append((f, max_length, shift_amount, output_path))\n",
        "    else:                            # if the input path is a directory\n",
        "        for filename in os.listdir(input_path):\n",
        "            f = os.path.join(input_path, filename)\n",
        "            if os.path.isfile(f):\n",
        "                files_to_process.append((f, max_length, shift_amount, output_path))\n",
        "\n",
        "\n",
        "    # process files in parallel using multiprocessing\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "    pool.map(process_file, files_to_process)\n",
        "    pool.close()\n",
        "    pool.join()"
      ],
      "metadata": {
        "id": "L5o3oju0A2VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_file_read(directory, filename):\n",
        "  '''\\\n",
        "  Determines whether or not a file has already been processed by checking if the output\n",
        "  filename exists in the output directory and has a size greater than 0.\n",
        "  '''\n",
        "  file_path = os.path.join(directory, filename.split('.')[0]  + '_segmented.csv')\n",
        "  if os.path.isfile(file_path) and os.path.getsize(file_path) > 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "QvBM9ughxBBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment(filepath, max_length, shift_amount, output_path):\n",
        "  filename = os.path.basename(filepath)\n",
        "  filename = filename.split('.')[0]\n",
        "\n",
        "  # process data to get sequences of appropriate length\n",
        "  if shift_amount is None:\n",
        "    df = preprocess_data(filepath, max_length)\n",
        "  else:\n",
        "    df = preprocess_shift(filepath, max_length, shift_amount)\n",
        "\n",
        "  # save output to csv\n",
        "  df.to_csv(output_path + '/' + filename + '_segmented.csv', encoding='utf-8', index=False, header=False)"
      ],
      "metadata": {
        "id": "TfBAZ02MCabD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(filepath, max_length):\n",
        "  records = []\n",
        "\n",
        "  f = filepath\n",
        "  if filepath.endswith('.gz'):\n",
        "    f = gzip.open(filepath, 'rt', encoding='utf-8')\n",
        "\n",
        "  try:\n",
        "    for record in SeqIO.parse(f, 'fasta'):\n",
        "      filename = os.path.basename(filepath)\n",
        "      name = filename.split('.')[0]\n",
        "      sample_ID = str(record.name)\n",
        "      seq = str(record.seq).upper()\n",
        "      pos = 0\n",
        "\n",
        "      # truncate sequences if longer than max_length\n",
        "      while len(seq) >= max_length:\n",
        "        records.append(\n",
        "          {\n",
        "            'name': name,\n",
        "            'sample_ID': sample_ID,\n",
        "            'start_position': pos,\n",
        "            'end_position': pos + max_length,\n",
        "            'length': max_length,\n",
        "            'sequence': seq[:max_length], # add subsequence up to max_length\n",
        "            'label': ''\n",
        "          }\n",
        "        )\n",
        "        seq = seq[max_length:] # sequence continuing from max_length\n",
        "        pos += max_length\n",
        "\n",
        "      # last case, for when max_length is None or len(seq) < max_length\n",
        "      records.append(\n",
        "          {\n",
        "            'name': name,\n",
        "            'sample_ID': sample_ID,\n",
        "            'start_position': pos,\n",
        "            'end_position': pos + len(seq),\n",
        "            'length': len(seq),\n",
        "            'sequence': seq,\n",
        "            'label': ''\n",
        "          }\n",
        "      )\n",
        "  finally:\n",
        "    df = pd.DataFrame(data=records)\n",
        "    return df"
      ],
      "metadata": {
        "id": "rO9UCdg1DFvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_shift(filepath, max_length, shift_amount):\n",
        "    records = []\n",
        "\n",
        "    f = filepath\n",
        "    if filepath.endswith('.gz'):\n",
        "        f = gzip.open(filepath, 'rt', encoding='utf-8')\n",
        "\n",
        "    try:\n",
        "        for record in SeqIO.parse(f, 'fasta'):\n",
        "          filename = os.path.basename(filepath)\n",
        "          name = filename.split('.')[0]\n",
        "          sample_ID = str(record.name)\n",
        "          seq = str(record.seq).upper()\n",
        "          pos = 0\n",
        "\n",
        "          while len(seq) >= max_length:\n",
        "              records.append(\n",
        "                  {\n",
        "                      'name': name,\n",
        "                      'sample_ID': sample_ID,\n",
        "                      'start_position': pos,\n",
        "                      'end_position': pos + max_length,\n",
        "                      'length': max_length,\n",
        "                      'sequence': seq[:max_length],\n",
        "                      'label': ''\n",
        "                  }\n",
        "              )\n",
        "              seq = seq[shift_amount:]  # shift the sequence by shift_amount\n",
        "              pos += shift_amount\n",
        "\n",
        "          records.append(\n",
        "              {\n",
        "                  'name': name,\n",
        "                  'sample_ID': sample_ID,\n",
        "                  'start_position': pos,\n",
        "                  'end_position': pos + len(seq),\n",
        "                  'length': len(seq),\n",
        "                  'sequence': seq,\n",
        "                  'label': ''\n",
        "              }\n",
        "          )\n",
        "          seq = seq[shift_amount:]  # shift the sequence by shift_amount\n",
        "    finally:\n",
        "      df = pd.DataFrame(data=records)\n",
        "      return df"
      ],
      "metadata": {
        "id": "B10M9srrElcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # Parameters\n",
        "  parser.add_argument(\n",
        "        \"--input\", default=None, type=str, required=True, help=\"The input directory or txt file list.\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "        \"--max_length\", default=None, type=int, required=True, help=\"The max sequence length for parsing\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "        \"--output\", default=None, type=str, required=False, help=\"The output directory.\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "        \"--shift_amount\", default=None, type=int, required=False, help=\"The amount of nucleotides to shift by when parsing\"\n",
        "  )\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  # read and format files\n",
        "  read_input(input_path=args.input,\n",
        "             max_length=args.max_length,\n",
        "             output_path=args.output,\n",
        "             shift_amount=args.shift_amount)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NeldsS9rGD-S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}