{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19jklDYM_X7zxuWsNd583LrsLCbJ7cvkD",
      "authorship_tag": "ABX9TyOTifBLHrxm1D1usrYnu9ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anihab/dnaTokenization/blob/main/vocabulary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install Bio\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "bYBKMF2UBuFo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c3cdc2-8369-431f-c6b4-b63528f77069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.6.2-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.8.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3099, in _parsed_pkg_info\n",
            "    return self._pkg_info\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _pkg_info. Did you mean: 'egg_info'?\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3119, in _compute_dependencies\n",
            "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3102, in _parsed_pkg_info\n",
            "    self._pkg_info = email.parser.Parser().parsestr(metadata)\n",
            "  File \"/usr/lib/python3.10/email/parser.py\", line 67, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "  File \"/usr/lib/python3.10/email/parser.py\", line 56, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.10/email/feedparser.py\", line 176, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.10/email/feedparser.py\", line 180, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.10/email/feedparser.py\", line 240, in _parsegen\n",
            "    self._parse_headers(headers)\n",
            "  File \"/usr/lib/python3.10/email/feedparser.py\", line 477, in _parse_headers\n",
            "    if line[0] in ' \\t':\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1612, in _log\n",
            "    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1549, in findCaller\n",
            "    def findCaller(self, stack_info=False, stacklevel=1):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#from Bio import SeqIO\n",
        "from tokenizers import Tokenizer, models, trainers, normalizers"
      ],
      "metadata": {
        "id": "DvTSAkKQIeDh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a BPE tokenizer on the full bacterial training directory to create the vocabulary file."
      ],
      "metadata": {
        "id": "a_pIqlUSHKOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 0\n",
        "\n",
        "define vocabulary size and input data path\n",
        "- input should be a path to a directory of bacterial fasta files"
      ],
      "metadata": {
        "id": "xKlq4YKA18Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals\n",
        "VOCAB_SIZE=4096\n",
        "INPUT_PATH=\"\"\n",
        "OUTPUT_PATH=\"\""
      ],
      "metadata": {
        "id": "A4o0BR8HOek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1\n",
        "\n",
        "parse through all fasta files in full bacterial training directory\n",
        "- to make input similar to input for pre-training, use sequences of at least 1500 nt (i.e., min sequence length = 1500)\n",
        "- do not limit max sequence length\n",
        "\n",
        "###Step 2\n",
        "\n",
        "build vocabulary json - limit vocab size to 4096"
      ],
      "metadata": {
        "id": "dQgmvQRhHbvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_iterator():\n",
        "  '''\\\n",
        "  iterates through the sequences of every csv file in a given directory\n",
        "  '''\n",
        "  if os.path.isdir(INPUT_PATH):\n",
        "        for filename in os.listdir(INPUT_PATH):\n",
        "          f = os.path.join(INPUT_PATH, filename)\n",
        "          with open(f, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "              # ensure the correct number of columns exists\n",
        "              if len(row) > 4:\n",
        "                seq = row[4]\n",
        "                # min sequence length = 1500\n",
        "                if len(seq) >= 1500:\n",
        "                  yield seq\n",
        "\n",
        "def build_vocab():\n",
        "  '''\\\n",
        "  builds a vocabulary on input\n",
        "  '''\n",
        "  tokenizer = Tokenizer(models.BPE())\n",
        "  tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
        "  trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE)\n",
        "  tokenizer.train_from_iterator(sequence_iterator(), trainer=trainer)\n",
        "  tokenizer.save(OUTPUT_PATH + \"/vocabulary_\" + VOCAB_SIZE + \".json\")"
      ],
      "metadata": {
        "id": "9dUis8uewwx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_vocab()"
      ],
      "metadata": {
        "id": "LUPTDqiNw0On"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}